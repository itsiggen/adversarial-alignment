# Adversarial Alignment

Adversarial Alignment is a framework for generating textual adversarial examples, developed for jailbreaking LLMs. We use Reinforcement Learning from Oracle Feedback (RLOF), an approach similar to alignment techniques, to adapt a generation policy towards learning how to create adversarial inputs. This approach is agnostic to the adversarial goal, the model under attack, as well as any and all defenses (like guardrails), thus general in its applicability.

<!-- ## Installation

Use the package manager [pip](https://pip.pypa.io/en/stable/) to install foobar.

```bash
pip install adal
``` -->

## Requirements
- Python 3.12.7

## Usage

```python
import adal

```

<!-- ## Contributing

Pull requests are welcome. For major changes, please open an issue first
to discuss what you would like to change.

Please make sure to update tests as appropriate.

## License

[MIT](https://choosealicense.com/licenses/mit/) -->