# Adversarial Alignment

Adversarial Alignment is a framework for generating textual adversarial examples, developed for jailbreaking LLMs. The approach employs SFT and alignment techniques (RLHF/GRPO) to adapt a generation policy, and is thus agnostic to the model under attack and the adversarial goal.

<!-- ## Installation

Use the package manager [pip](https://pip.pypa.io/en/stable/) to install foobar.

```bash
pip install adal
``` -->

## Requirements
- Python 3.12.7

## Usage

```python
import adal

```

<!-- ## Contributing

Pull requests are welcome. For major changes, please open an issue first
to discuss what you would like to change.

Please make sure to update tests as appropriate.

## License

[MIT](https://choosealicense.com/licenses/mit/) -->