# Adversarial Alignment 

## Overview

Adversarial Alignment is a framework for generating textual adversarial examples, developed for jailbreaking LLMs. We use Reinforcement Learning from Oracle Feedback (RLOF), an approach similar to alignment techniques, to adapt a generation policy towards learning how to create adversarial inputs. This approach is agnostic to the adversarial goal, the model under attack, as well as any and all defenses (like guardrails), thus general in its applicability.

## Project Structure

- **SFT**: For the supervised fine-tuning process.
  - `gen_dataset.py`: To create a SFT dataset using data from JailbreakBench.
  - `sft.py`: Supervised fine-tuning for the LLM.

- **RL**: for reinforcement learning.
  - `rewards.py`: For reward functions for evaluating model outputs.
  - `rlvr.py`: Contains the main RL algorithm implementation.

- **RL_EVAL**: Provides a FastAPI service for evaluating the jailbreaks as a reward for RL.
  - `app.py`: Entry point for the FastAPI service, setting up API endpoints.



## Usage

### Supervised Fine Tuning

To generate datasets and train the model using SFT, run the following command:

```bash
python SFT/sft.py
```

### Reinforcement Learning



To evaluate the jailbreaks generated by the model, start the FastAPI server:

```bash
python RL_EVAL/app.py  
```

The API will be available at `http://localhost:8000`.

To train the model using reinforcement learning, execute:

```bash
python RL/rlvr.py
```
